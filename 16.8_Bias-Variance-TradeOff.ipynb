{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c467ed1a",
   "metadata": {},
   "source": [
    "# Biasâ€“Variance Trade-Off \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In Machine Learning, the objective of a model is to **generalize well** to unseen data.\n",
    "Prediction error arises due to three main components:\n",
    "\n",
    "- Bias\n",
    "- Variance\n",
    "- Irreducible Noise\n",
    "\n",
    "The **Biasâ€“Variance Trade-Off** explains why increasing model complexity does not always improve performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bias\n",
    "\n",
    "### Definition\n",
    "> **Bias** is a source of error introduced by making **simplifying assumptions** about the data.\n",
    "\n",
    "A high-bias model:\n",
    "- Assumes the relationship is very simple\n",
    "- Fails to capture the true underlying pattern\n",
    "\n",
    "### Characteristics\n",
    "- High training error\n",
    "- High testing error\n",
    "- Model is rigid and inflexible\n",
    "\n",
    "### Example\n",
    "Using **simple linear regression** for a non-linear dataset.\n",
    "\n",
    "### Outcome\n",
    "âž¡ï¸ **Underfitting**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Variance\n",
    "\n",
    "### Important Clarification\n",
    "In the biasâ€“variance trade-off, **variance does NOT mean spread of the data**.\n",
    "\n",
    "### Definition\n",
    "> **Variance = how much the model changes when the training data changes slightly**<br>\n",
    "> **Variance** is the source of error caused when the model being **highly sensitive to small changes in the training dataset**.\n",
    "\n",
    "A high-variance model:\n",
    "- Fits training data extremely well\n",
    "- Changes significantly if trained on a slightly different dataset\n",
    "- Learns noise as if it were signal\n",
    "\n",
    "### Characteristics\n",
    "- Very low training error\n",
    "- High testing error\n",
    "- Highly flexible model\n",
    "\n",
    "### Example\n",
    "High-degree polynomial regression.\n",
    "\n",
    "### Outcome\n",
    "âž¡ï¸ **Overfitting**\n",
    "> **High variance leads to overfitting because the model is too sensitive to small changes in training data and starts learning noise instead of the underlying pattern.**\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Two Meanings of Variance (Critical Concept)\n",
    "\n",
    "### A. Statistical Variance (Data Variance)\n",
    "\n",
    "- Property of the **dataset**\n",
    "- Measures how spread-out the data values are\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "\\text{Var}(X) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### B. Model Variance (ML Variance)\n",
    "\n",
    "- Property of the **model**\n",
    "- Measures how much the model predictions vary when trained on different datasets drawn from the same population\n",
    "\n",
    "ðŸ“Œ These two variances are **conceptually different**, even though they share the same name.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Signal vs Noise Analogy\n",
    "\n",
    "### Origin\n",
    "- Derived from **World War II radar detection systems**\n",
    "- Radar had to distinguish:\n",
    "  - Signal â†’ enemy aircraft\n",
    "  - Noise â†’ random interference\n",
    "\n",
    "### In Machine Learning\n",
    "- **Signal** â†’ true underlying pattern in data\n",
    "- **Noise** â†’ random fluctuations and measurement errors\n",
    "\n",
    "### Key Idea\n",
    "> High-variance models cannot distinguish between signal and noise and therefore learn both.\n",
    "\n",
    "This directly leads to **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Linear Regression Example\n",
    "\n",
    "### Case 1: Simple Linear Regression\n",
    "\n",
    "Model:\n",
    "$$\n",
    "y = w_0 + w_1x\n",
    "$$\n",
    "\n",
    "- Few parameters\n",
    "- Limited flexibility\n",
    "- Stable predictions across different datasets\n",
    "\n",
    "âž¡ï¸ **High bias, low variance**\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2: High-Degree Polynomial Regression\n",
    "\n",
    "Model:\n",
    "$$\n",
    "y = w_0 + w_1x + w_2x^2 + \\dots + w_nx^n\n",
    "$$\n",
    "\n",
    "- Many parameters\n",
    "- Extremely flexible\n",
    "- Small data changes cause large changes in model behavior\n",
    "\n",
    "âž¡ï¸ **Low bias, high variance**\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why High Variance Causes Overfitting\n",
    "\n",
    "- The model has too many degrees of freedom\n",
    "- It fits:\n",
    "  - True pattern (signal)\n",
    "  - Random fluctuations (noise)\n",
    "- Noise does not generalize to unseen data\n",
    "\n",
    "Result:\n",
    "- Training error decreases\n",
    "- Testing error increases\n",
    "\n",
    "âž¡ï¸ **Overfitting**\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Mathematical Intuition (Linear Regression)\n",
    "\n",
    "Prediction:\n",
    "$$\n",
    "\\hat{y} = Xw\n",
    "$$\n",
    "\n",
    "Weight estimation:\n",
    "$$\n",
    "w = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "- High model complexity â†’ unstable $X^TX$\n",
    "- Small change in $y$ or $X$ â†’ large change in $w$\n",
    "\n",
    "âž¡ï¸ **High variance**\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Biasâ€“Variance Trade-Off Table\n",
    "\n",
    "| Model Complexity | Bias | Variance | Result |\n",
    "|-----------------|------|----------|--------|\n",
    "| Very Simple | High | Low | Underfitting |\n",
    "| Moderate | Balanced | Balanced | Best Generalization |\n",
    "| Very Complex | Low | High | Overfitting |\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Biasâ€“Variance Decomposition\n",
    "\n",
    "Expected prediction error can be decomposed as:\n",
    "$$\n",
    "\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\text{Bias}^2$ â†’ error from incorrect assumptions\n",
    "- $\\text{Variance}$ â†’ error from sensitivity to training data\n",
    "- $\\text{Noise}$ â†’ unavoidable randomness\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Exam-Ready One-Liners\n",
    "\n",
    "- **Bias**: Error due to overly simple assumptions.\n",
    "- **Variance**: Error due to excessive sensitivity to training data.\n",
    "- **High bias â†’ underfitting**\n",
    "- **High variance â†’ overfitting**\n",
    "- **Variance in ML â‰  variance of data**\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Controlling Bias and Variance\n",
    "\n",
    "### To Reduce Bias:\n",
    "- Increase model complexity\n",
    "- Add relevant features\n",
    "- Use non-linear models\n",
    "\n",
    "### To Reduce Variance:\n",
    "- Increase training data\n",
    "- Apply regularization (Ridge, Lasso)\n",
    "- Reduce model complexity\n",
    "- Use ensemble methods (Bagging)\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Final Golden Statement\n",
    "\n",
    "> **The biasâ€“variance trade-off is the balance between learning too little (bias) and learning too much noise (variance) to achieve optimal generalization.**\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
